# -*- coding: utf-8 -*-
"""Chapter 04 - 오늘 밤 유럽 축구, 어디가 이길까_ 데이터로 분석하고 내기를 이겨보자(문제).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DoSdgQ4YzlZdk0fQ6sZuPTJ4S98CwQfd

# 주제 : 오늘 밤 유럽 축구, 어디가 이길까? 데이터로 분석하고 내기를 이겨보자!
----------

## 실습 가이드
    1. 데이터를 다운로드하여 Colab에 불러옵니다.
    2. 필요한 라이브러리는 모두 코드로 작성되어 있습니다.
    3. 코드는 위에서부터 아래로 순서대로 실행합니다.
    
    
## 데이터 소개
    - 이번 주제는 European Soccer Database 데이터셋을 사용합니다.
    
    - 다음 1개의 sqlite 데이터베이스를 사용합니다.
    database.sqlite

    - 데이터 베이스 내 총 7개의 Table을 사용합니다.
    Country: 국가 정보
    League: 리그 정보
    Match: 경기 정보 (주 데이터셋)
    Player: 플레이어 정보
    Player_Attributes: 플레이어의 특성
    Team: 팀 정보
    Team_Attributes: 팀의 특성
    
- 데이터 출처: https://www.kaggle.com/hugomathien/soccer

## 최종 목표
    - SQL 데이터셋에서 테이블을 읽어들이는 방법 이해
    - 여러개의 테이블을 통합하는 방법 이해
    - 수 많은 데이터로부터 관심있는 데이터를 식별
    - 학습된 모델로부터 인사이트 습득 방법 이해

- 출제자 : 신제용 강사
---

## Step 0. 데이터베이스와 SQL

### 데이터베이스란
RDB (관계형 DB) : 컬럼간 상관성이 잇는 데이터끼리 엮어 놓은것 표로 되있다 (MYSQL, Oracle) 등이 있다

noSQL (비관계형 DB) : 비정형 데이터, 표가아닌 json형태로 되어있다. 특수한 경우는 json을 쓰는게 더 효율적이기도 함

### SQL과 Query
SQL (Structured Query Language): 관계형 데이터 베이스를 다룰때 쓴다.
- 영구적으로 저장할 저장소에 접근해서 하는방법
- In-memory (H2) RAM에다가 저장소를 두고 데이터를 다룸
- SQLite

## Step 1. 데이터셋 준비하기
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""### 문제 1. Colab Notebook에 Kaggle API 세팅하기

"""

import os

# os.environ을 이용하여 Kaggle API Username, Key 세팅하기
os.environ['KAGGLE_USERNAME'] = 'samjinwang'
os.environ['KAGGLE_KEY'] = 'e808743bea20e4a89b105256b9d5941b'

"""### 문제 2. 데이터 다운로드 및 압축 해제하기

"""

# Linux 명령어로 Kaggle API를 이용하여 데이터셋 다운로드하기 (!kaggle ~)
# Linux 명령어로 압축 해제하기
!kaggle datasets download -d hugomathien/soccer
!unzip '*.zip'

"""### 문제 3. sqlite3와 Pandas로 sqlite 데이터베이스 읽어들이기

"""

import sqlite3

# sqlite3.connect()와 pd.read_sql_query()로 csv파일 읽어들이기
conn = sqlite3.connect('database.sqlite')
df_country = pd.read_sql_query('SELECT * from Country', conn)
df_league = pd.read_sql_query('SELECT * from League', conn)
df_match = pd.read_sql_query('SELECT * from Match', conn)
df_player = pd.read_sql_query('SELECT * from Player', conn)
df_player_att = pd.read_sql_query('SELECT * from Player_Attributes', conn)
df_team = pd.read_sql_query('SELECT * from Team', conn)
df_team_att = pd.read_sql_query('SELECT * from Team_Attributes', conn)

"""## Step 2. EDA 및 데이터 기초 통계 분석

### 문제 4. 각 데이터프레임의 구조 파악하기
"""

# DataFrame에서 제공하는 메소드를 이용하여 각 데이터프레임의 구조 분석하기 (head(), info(), describe())
df_country.head()
#국가 아이디랑 국가 이름

df_league.head()
# 리그 아이디, 국가 아이디, 리그 이름

# 1대1 매칭인지확인
# 모두 11개이다
df_league.info()

df_league['id'].unique() #11개임 -> 각국가별로 하나의 리그만 있다

df_match.head()
#column 확인: 배당관련, goal, shoton,shoff등등 goal 이후 컬럼들은 제외하고 본다

for c, num in zip(df_match.columns, df_match.isna().sum()):
  print (c, num)
#df_match.describe하니까 너무 길어서 못봄
#team 에 관련된건 다 데이터가 차있다
# player 와 관련된건 최대 1832개만큼 데이터가 비어있다
# 홈 플ㄹㅠ

df_match.drop(df_match.columns[-38:], axis=1, inplace=True)

df_match.head()

df_match.loc[df_match['league_id']==1729] #국가 아이디가 1729(잉글랜드) 의 매치 정보 가져오기

df_match['season'].unique() #다행히 일정한 구간으로 연도가 붙여져 있다



df_player.head()

df_player_att.head()

df_team.head()

df_team_att.head()

"""### 문제 5. 데이터프레임간의 관계 파악하기

"""

# 데이터프레임 간 중복되는 Column이 있는지 확인하고 유용한 Column 식별하기
# Hint) unique()로 값을 비교하거나, map() 등을 활용하여 Column 관계를 확인
df_player_att['player_api_id'].value_counts() #중복되는 데이터가 있는지 본다
# 플레이당 최소 두번 쓰여져 있고 어떤 선수는 56번도 쓰임 -> 각 플레이어아이디 별로 평균내서 한다

df_match['away_player_1']
# Nan 데이터도 있고 0~70만까지 데이터가 잇다. 첫 100000구간까지 대부분의 데이터가 있다
# float 형태임
df_match['home_player_1'].dropna().apply(int).map(df_player_att.groupby('player_api_id').mean()['overall_rating']).isna().sum() #series형태
#아이디별로 overall rating이 됨
# isna().sum() 해보면 0이됨. -> 'away_player_1' 데이터 안에 플레이어 아이디가 df_player_att에 다 있다

df_team_att['team_api_id'].value_counts()
df_match['away_team_api_id'].map(df_team_att.groupby('team_api_id').mean()['buildUpPlaySpeed']).isna().sum()

"""### 문제 6. 선수 특성 사이의 상관성 파악하기"""

# DataFrame의 corr() 메소드와 Seaborn의 heatmap() 메소드를 이용하여 Pearson's correlation 시각화하기
fig = plt.figure(figsize = (8,8))
sns.heatmap(df_player_att.drop(['id','player_fifa_api_id','player_api_id'],axis=1).corr())
sns.heatmap(df_player_att.drop(['id','player_fifa_api_id','player_api_id'],axis=1).corr()[['overall_rating']],annot=True)
#밝은부분이 연관성 높은것
# overall_rating이랑 potentiaㅣ 이랑 상관성이 높아보임
# reaction과 overall_rating이랑 연관성이 높아보임
# shot_power이랑 #long_shots 과 연관성이 높아보임
# 선수가 어떤것을 잘해야 좋은평가를 받는지 볼수있다



"""### 문제 7. 매치 데이터프레임에 팀 특성 데이터프레임 통합하기

"""

df_team_att.columns

# DataFrame의 map() 메소드를 활용하여 데이터프레임 통합하기
df_team_att.columns #일부는 뉴메릭 일부는 카테고리칼
df_team_att.info() #dribbling 데이터가 많이 빠짐
df_team_att.drop('buildUpPlayDribbling',axis = 1, inplace = True)

#team api id 도 중복값이 많아서 묶어줘야함
#근데 평균값으로 묶기엔 범주형데이터도 있음
most = lambda x: x.value_counts().index[0]
#같은 아이디안에서 범주형데이터를 각각 카운트하고
#그 아이디에서 가장많이 카운트된 범주형 데이터를  
#같은 아이디끼리 묶을때 값으로 한다



team_map = df_team_att.groupby('team_api_id').aggregate(
    {
       'buildUpPlaySpeed': 'mean',
       'buildUpPlaySpeedClass': most,
       'buildUpPlayDribblingClass': most,
       'buildUpPlayPassing': 'mean',
       'buildUpPlayPassingClass': most,
       'buildUpPlayPositioningClass': most,
       'chanceCreationPassing': 'mean',
       'chanceCreationPassingClass': most,
       'chanceCreationCrossing': 'mean',
       'chanceCreationCrossingClass': most,
       'chanceCreationShooting': 'mean',
       'chanceCreationShootingClass': most,
       'chanceCreationPositioningClass': most,
       'defencePressure': 'mean',
       'defencePressureClass': most,
       'defenceAggression': 'mean',
       'defenceAggressionClass': most,
       'defenceTeamWidth': 'mean',
       'defenceTeamWidthClass': most,
       'defenceDefenderLineClass': most
    }
)

team_map #각 id 별로 aggregation 잘됨

df = df_match[['home_team_goal', 'away_team_goal']].copy()

#위에서 만든 데이터에 데이터 정보 집어넣기
df = df_match[['home_team_goal', 'away_team_goal']].copy()
for team in ['home_', 'away_']:
  team_map.index.name = team + 'team_api_id'
  for col in team_map.columns:
    df[team + col] = df_match[team_map.index.name].map(team_map[col])

df.dropna(inplace = True)

df.head()

"""### 문제 8. 홈과 어웨이의 골 수를 승-무-패 범주로 변환하기"""

# 홈과 어웨이의 골 수를 범주형 데이터로 변환하기 (0: 홈팀 승, 1: 무승부, 2: 어웨이팀 승)
df['matchResult'] = df[['home_team_goal','away_team_goal']].aggregate(lambda x : 0 if x[0] > x[1] else 1 if x[0] == x[1] else 2, axis=1)

df.drop(['home_team_goal','away_team_goal'],axis =1, inplace = True)

"""## Step 3. 모델 학습을 위한 데이터 전처리

### 문제 9. get_dummies를 이용하여 범주형 데이터 전처리하기
"""

#범주형 데이터는 'class'라고 쓰여져 있음
#'class'가 포함된 컬럼끼리 묶어서 범주형 데이터용 데이터셋을 만듬
col_cats = list(filter(lambda s: s.find('Class') >= 0, df.columns))
df_cats= pd.get_dummies(df[col_cats],drop_first = True)
df_cats

"""### 문제 10. StandardScaler를 이용해 수치형 데이터 표준화하기

"""

from sklearn.preprocessing import StandardScaler

# StandardScaler를 이용해 수치형 데이터를 표준화하기
# Hint) Multicollinearity를 피하기 위해 불필요한 컬럼은 drop한다.

X_num = df.drop(['matchResult'] + col_cats, axis = 1)
scalar = StandardScaler()
scalar.fit(X_num)
X_scaled = scalar.transform(X_num)
X_scaled = pd.DataFrame(data=X_scaled, index=X_num.index, columns
                        =X_num.columns)


X_cat = df_cats
X = pd.concat([X_scaled, X_cat], axis = 1)
y = df['matchResult']

X.info()

"""### 문제 11. 학습데이터와 테스트데이터 분리하기

"""

from sklearn.model_selection import train_test_split

# train_test_split() 함수로 학습 데이터와 테스트 데이터 분리하기
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state =1)

"""## Step 4. Classification 모델 학습하기

### 문제 12. Logistic Regression 모델 생성/학습하기
"""

from sklearn.linear_model import LogisticRegression

# LogisticRegression 모델 생성/학습
model_lr = LogisticRegression(max_iter = 10000)
model_lr.fit(X_train,y_train)

from sklearn.metrics import classification_report



pred = model_lr.predict(X_test)
print(classification_report(y_test,pred))

print(sum(y_test == 0) / len(y_test)) #test 데이터 중에 홈팀이 이긴경우(0)은 0.45인데
#학습해서 나온것은 홈팀이 이긴것의 비율이 0.48
print(sum(y_test == 1) / len(y_test))
print(sum(y_test == 2) / len(y_test))

"""### 문제 13. 모델 학습 결과 평가하기

"""

from sklearn.metrics import classification_report

# Predict를 수행하고 classification_report() 결과 출력하기
pred =

"""### 문제 14. XGBoost 모델 생성/학습하기

"""

from xgboost import XGBClassifier

# XGBClassifier 모델 생성/학습
model_xgb = XGBClassifier()
model_xgb.fit(X_train,y_train)

"""### 문제 15. 모델 학습 결과 평가하기

"""

# Predict를 수행하고 classification_report() 결과 출력하기
pred = model_xgb.predict(X_test)

print(classification_report(y_test, pred))
#절반 넘게 맞춤 -> 50프로이상이면 결국 더 많이 맞춘쪽으로 잘됨

"""## Step5 모델 학습 결과 심화 분석하기

### 문제 16. Logistic Regression 모델 계수로 상관성 파악하기
"""

# Logistic Regression 모델의 coef_ 속성을 plot하기
plt.figure(figsize = (15,5))
plt.plot(X.columns, model_lr.coef_[0])
plt.xticks(rotation=90)
plt.title('What makes home team win')
plt.grid()
plt.show()
#away_defense pressure 가 높은 영향: away 에서 수비를 강하게 하려고 하면 홈팀이 이김
#away가 크로스를 잘 못만드는 팀이면 홈팀이 이기는거에 영향

plt.figure(figsize = (15,5))
plt.plot(X.columns, model_lr.coef_[2])
plt.xticks(rotation=90)
plt.title('What makes home team lose')
plt.grid()
plt.show()

"""### 문제 17. XGBoost 모델로 특징의 중요도 확인하기"""

# XGBoost 모델의 feature_importances_ 속성을 plot하기
plt.figure(figsize = (15,5))

plt.bar(X.columns, model_xgb.feature_importances_)
plt.xticks(rotation=90)
plt.grid()
plt.show()
#모델이 예측시 카테고리컬 데이터에서 힌트를 많이 얻는 모양임
#수치형 데이터들은 별로 영향을 주지 못함

"""## Step6 모델 성능 개선하기

### 문제 18. 매치 데이터프레임에 선수 특성 데이터프레임 통합하기
"""

# 선수 특성 중 유의미한 정보를 매치 데이터프레임에 통합하기

# 선수 특성 중 유의미한 정보를 매치 데이터프레임에 통합하기
df = df_match[['home_team_goal', 'away_team_goal']].copy()

for team in ['home_', 'away_']:
  team_map.index.name = team + 'team_api_id'
  for col in team_map.columns:
    df[team + col] = df_match[team_map.index.name].map(team_map[col])

df_player_att.head()

player_map = df_player_att.groupby('player_api_id').mean()['overall_rating']
player_map

for col in (s + str(idx) for s in ['home_player_', 'away_player_'] for idx in range(1,12)):
  df[col +'_rating'] = df_match[col].map(player_map)

df.isna().sum().max() #nan값 1555 전체 25000개니까 드랍해도 될듯

df.dropna(inplace=True) #nan값있는 row들 다 제거

df.head() #nan 값없는거 확인

df.isna().sum().max()

"""### 문제 19. 모델 재학습하여 평가하기"""

df['matchResult'] = df[['home_team_goal','away_team_goal']].aggregate(lambda x : 0 if x[0] > x[1] else 1 if x[0] == x[1] else 2, axis=1)
df.drop(['home_team_goal','away_team_goal'],axis =1, inplace = True)

df.columns

col_cats

# StandardScaler를 이용해 수치형 데이터를 표준화하기
# Hint) Multicollinearity를 피하기 위해 불필요한 컬럼은 drop한다.

X_num = df.drop(['matchResult'] + col_cats, axis = 1)
scalar = StandardScaler()
scalar.fit(X_num)
X_scaled = scalar.transform(X_num)
X_scaled = pd.DataFrame(data=X_scaled, index=X_num.index, columns
                        =X_num.columns)


X_cat = pd.get_dummies(df[col_cats], drop_first = True)
X = pd.concat([X_scaled, X_cat], axis = 1)
y = df['matchResult']

# train_test_split() 함수로 학습 데이터와 테스트 데이터 분리하기
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state =1)



# LogisticRegression 모델 생성/학습
model_lr = LogisticRegression(max_iter = 10000)
model_lr.fit(X_train,y_train)

from sklearn.metrics import classification_report



pred = model_lr.predict(X_test)
print(classification_report(y_test,pred))

#0.53으로 정확도가 상승되었다!


plt.figure(figsize = (15,5))
plt.plot(X.columns, model_lr.coef_[0])
plt.xticks(rotation=90)
plt.title('What makes home team win')
plt.grid()
plt.show()
#여전히 categoical 데이터가 우세
# overall_rating관련 column들이 팀에대한 숫자들에 비하면 더 높은 영향력이 있다
# away 팀 선수들이 좋으면 결과가 패쪽으로 감(당연)

from xgboost import XGBClassifier

# XGBClassifier 모델 생성/학습
model_xgb = XGBClassifier()
model_xgb.fit(X_train,y_train)

"""### 문제 15. 모델 학습 결과 평가하기

"""

# Predict를 수행하고 classification_report() 결과 출력하기
pred = model_xgb.predict(X_test)

print(classification_report(y_test, pred))
#절반 넘게 맞춤 -> 50프로이상이면 결국 더 많이 맞춘쪽으로 잘됨

# XGBoost 모델의 feature_importances_ 속성을 plot하기
plt.figure(figsize = (15,5))

plt.bar(X.columns, model_xgb.feature_importances_)
plt.xticks(rotation=90)
plt.grid()
plt.show()
#팀간의 상성보다
#선수들의 능력이 더 중요하다고 나옴 팀의 중요도가 엄청 낮음
# 플레이어에 대한 정보만 넣어줘도 비슷한 accuracy를 나올수도 있을듯

#한번에 가장좋은 모델을 만드려고 안해도됨
#엄청 규모가 커서 한번에 다하는게 불가능하다
#고객이 원하는 정보를 원하는 시점안에 줘야하기때문에 고객 만족도를 위해 적절한 수준에서 일단 모델을 만들어놓고 개선하는 식으로 한다
#그리고 베이스라인 (최소성능을 보장해놓고) 이후에 더 발전 가능성을 찾아가는 과정이 중요
# MULTICOLLINEARITY 등 오류를 발견해 가면서